{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SFT (Supervised Fine-Tuning, instruction-tuning)**  — метод адаптации предварительно обученных языковых моделей (LLM) под конкретную задачу с помощью размеченных данных\n",
    "\n",
    "**Цель** — скорректировать веса модели так, чтобы она лучше справлялась с задачей, не теряя при этом общие знания, полученные в ходе предварительного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка зависимотсей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate\n",
    "!huggingface-cli login\n",
    "!pip install --upgrade pip\n",
    "!pip install --force-reinstall \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --force-reinstall unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model SQLCoder-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"defog/sqlcoder-7b\"\n",
    "\n",
    "# загрузка модели\n",
    "model_sql, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# оптимизация под инференс\n",
    "# FastLanguageModel.for_inference(model_sql)\n",
    "\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# You are an SQL expert.\n",
    "# Generate a query to find the top 5 customers with the highest total orders.\n",
    "# Tables:\n",
    "# orders(order_id, customer_id, order_date, total)\n",
    "# customers(customer_id, name)\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# # генерация\n",
    "# _ = model.generate(\n",
    "#     **inputs,\n",
    "#     streamer=streamer,\n",
    "#     max_new_tokens=200\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/AQUA/data_processing/data/queries_marked_602_915.jsonl\")\n",
    "\n",
    "full_dataset = dataset[\"train\"]\n",
    "\n",
    "temp_split = full_dataset.train_test_split(test_size=0.2, seed=42, shuffle=True)\n",
    "train_dataset = temp_split[\"train\"]\n",
    "temp = temp_split[\"test\"]\n",
    "\n",
    "val_test = temp.train_test_split(test_size=0.5, seed=42, shuffle=True)\n",
    "\n",
    "eval_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]\n",
    "print(train_dataset[0])\n",
    "print(len(train_dataset), len(eval_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lora adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ddecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model_sql,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=5, #num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-6, \n",
    "    optim=\"adamw_torch_fused\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=10,\n",
    "    eval_steps=25,\n",
    "    save_steps=25,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\", # Будем менять\n",
    "    warmup_ratio=0.2,\n",
    "    seed=3407,\n",
    "    output_dir=\"/content/AQUA/fine-tuning/training_args\",\n",
    "    logging_dir=\"/content/AQUA/fine-tuning/logs\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and launching training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(example):\n",
    "    prompt = example[\"instruction\"]\n",
    "    if example[\"input\"].strip() != \"\":\n",
    "        prompt += \"\\nInput: \" + example[\"input\"]\n",
    "    prompt += \"\\nOutput: \" + example[\"output\"]\n",
    "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=False)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/AQUA/fine-tuning/Model/sqlcoder-finetuned\")\n",
    "tokenizer.save_pretrained(\"/content/AQUA/fine-tuning/Model/sqlcoder-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
